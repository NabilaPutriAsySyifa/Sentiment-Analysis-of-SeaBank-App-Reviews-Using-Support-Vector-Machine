# -*- coding: utf-8 -*-
"""Sentiment Analysis Sea Bank Algoritma SVM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1136SqWCV7yi-ebzd4h2ZcPa2mf7YcdTY

# **Preparation**
"""

import pandas as pd
pd.options.mode.chained_assignment = None
import numpy as np
seed = 0
np.random.seed(seed)
import matplotlib.pyplot as plt
import seaborn as sns

import datetime as dt
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import warnings
warnings.filterwarnings('ignore')

# Install library Sastrawi untuk processing teks bahasa Indonesia
!pip install sastrawi

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

from wordcloud import WordCloud

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('punkt_tab')
nltk.download('stopwords')

# Install gensim
!pip install gensim

"""# **Data Wrangling**

## **Data Gathering**
"""

# Load dataset ulasan aplikasi
# Membuat DataFrame dari hasil scrapreview
df = pd.read_csv('https://drive.google.com/uc?id=1-3DRes9fkWgTirT6LD9G2uFMAyGD_-TX')

# Menghitung jumlah baris dan kolom dalam DataFrame
jumlah_ulasan, jumlah_kolom = df.shape

# Mengambil data sebanyak 15000
app_reviews_df = df.head(15000)

"""## **Data Assessing**"""

# Cek dataset ulasan ruangguru
app_reviews_df.head()

# Cek informasi dataset
app_reviews_df.info()

"""## **Data Cleaning**"""

# Membuat DataFrame baru (clean_df) dengan menghapus baris yang memiliki nilai yang hilang (NaN) dari app_reviews_df
clean_df = app_reviews_df.dropna()

# Menghapus baris duplikat dari DataFrame clean_df
clean_df = clean_df.drop_duplicates()

# Menghitung jumlah baris dan kolom dalam DataFrame clean_df setelah menghapus duplikat
jumlah_ulasan_setelah_hapus_duplikat, jumlah_kolom_setelah_hapus_duplikat = clean_df.shape

print(f'Jumlah ulasan setelah menghapus duplikat: {jumlah_ulasan_setelah_hapus_duplikat}')
print(f'Jumlah kolom setelah menghapus duplikat: {jumlah_kolom_setelah_hapus_duplikat}')

# Cek informasi data ulasan terbaru setelah cleaning
clean_df.info()

"""# **Text Processing**"""

# Fungsi untuk menghapus huruf, angka, tanda baca, spasi, dan semacamnya
def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention
    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag
    text = re.sub(r'RT[\s]', '', text) # menghapus RT
    text = re.sub(r"http\S+", '', text) # menghapus link
    text = re.sub(r'[0-9]+', '', text) # menghapus angka
    text = re.sub(r'[^\w\s]', '', text) # menghapus karakter selain huruf dan angka

    text = text.replace('\n', ' ') # mengganti baris baru dengan spasi
    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca
    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks
    return text

# Fungsi untuk mengubah format huruf menjadi kecil
def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil
    text = text.lower()
    return text

# Fungsi untuk tokenisasi
def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token
    text = word_tokenize(text)
    return text

# Fungsi untuk menghapus stopwords dalam kalimat
def filteringText(text): # Menghapus stopwords dalam teks
    listStopwords = set(stopwords.words('indonesian'))
    listStopwords1 = set(stopwords.words('english'))
    listStopwords.update(listStopwords1)
    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',"di","ga","ya","gaa","loh","kah","woi","woii","woy"])
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    text = filtered
    return text

# Fungsi untuk menghapus imbuhan dalam teks agar berubah menjadi bentuk dasar
def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata
    # Membuat objek stemmer
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()

    # Memecah teks menjadi daftar kata
    words = text.split()

    # Menerapkan stemming pada setiap kata dalam daftar
    stemmed_words = [stemmer.stem(word) for word in words]

    # Menggabungkan kata-kata yang telah distem
    stemmed_text = ' '.join(stemmed_words)

    return stemmed_text

# Fungsi untuk mengubah daftar kata ke dalam kalimat
def toSentence(list_words): # Mengubah daftar kata menjadi kalimat
    sentence = ' '.join(word for word in list_words)
    return sentence

# Menghapus slang words pada data teks
slangwords = {"@": "di", "abis": "habis", "wtb": "beli", "masi": "masih", "wts": "jual",
              "wtt": "tukar", "bgt": "banget", "maks": "maksimal", "w": "aku",
              "ama" : "sama", "lo" : "kamu"}

# Path ke file slang_indo.txt untuk menambah pembendaharaan kata
slang_file_url = 'https://drive.google.com/uc?id=1ZvezExew8-jylnahP_Nb6uG9wNCsInMD&export=download' # Tambahkan &export=download
local_file_path = 'slang_indo.txt'

!wget "{slang_file_url}" -O "{local_file_path}"

# Muat data slang dari file
try:
    with open(local_file_path, 'r', encoding='utf-8') as file:
        for line in file:
            # Format "slang:baku" per baris
            parts = line.strip().split('\t')
            if len(parts) == 2:
                slang, baku = parts
                slangwords[slang.lower()] = baku.lower() # Tambahkan ke kamus, konversi ke lowercase
except FileNotFoundError:
    print(f"File '{local_file_path}' tidak ditemukan. Hanya menggunakan kamus bawaan.")
except Exception as e:
    print(f"Terjadi kesalahan saat membaca file '{local_file_path}': {e}")

def fix_slangwords(text):
    words = text.split()
    fixed_words = []

    for word in words:
        if word.lower() in slangwords:
            fixed_words.append(slangwords[word.lower()])
        else:
            fixed_words.append(word)

    fixed_text = ' '.join(fixed_words)
    return fixed_text

print(slangwords)

# Contoh hasil dari slang word
teks_dengan_slang = "ihh jijay gw ama lo"
teks_tanpa_slang = fix_slangwords(teks_dengan_slang)
print(teks_tanpa_slang)

# Membersihkan teks dan menyimpannya di kolom 'text_clean'
clean_df['text_clean'] = clean_df['content'].apply(cleaningText)

# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'
clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText)

# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'
clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords)

# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'
clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)

# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'
clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)

# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'
clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)

# Cek dataset review setelah di-preprocessing
clean_df.head()

"""# **Labeling**"""

import requests
import csv
from io import StringIO

# Membaca data kamus kata-kata positif dari GitHub
lexicon_positive = dict()

response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')
# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub

if response.status_code == 200:
    # Jika permintaan berhasil
    reader = csv.reader(StringIO(response.text), delimiter=',')
    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma

    for row in reader:
        # Mengulangi setiap baris dalam file CSV
        lexicon_positive[row[0]] = int(row[1])
        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive
else:
    print("Failed to fetch positive lexicon data")

# Membaca data kamus kata-kata negatif dari GitHub
lexicon_negative = dict()

response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')
# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub

if response.status_code == 200:
    # Jika permintaan berhasil
    reader = csv.reader(StringIO(response.text), delimiter=',')
    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma

    for row in reader:
        # Mengulangi setiap baris dalam file CSV
        lexicon_negative[row[0]] = int(row[1])
        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative
else:
    print("Failed to fetch negative lexicon data")

# Fungsi untuk menentukan polaritas sentimen dari tweet
def sentiment_analysis_lexicon_indonesia(text):
    score = 0
    # Inisialisasi skor sentimen ke 0

    for word in text:
        # Mengulangi setiap kata dalam teks

        if (word in lexicon_positive):
            score = score + lexicon_positive[word]
            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen

    for word in text:
        # Mengulangi setiap kata dalam teks (sekali lagi)

        if (word in lexicon_negative):
            score = score + lexicon_negative[word]
            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen

    polarity=''
    # Inisialisasi variabel polaritas

    if (score > 0):
        polarity = 'positive'
        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif
    elif (score < 0):
        polarity = 'negative'
        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif

    else:
        polarity = 'neutral'
        # Jika tidak keduanya, maka netral

    return score, polarity
    # Mengembalikan skor sentimen dan polaritas teks

# Cek hasil polaritas
results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)
results = list(zip(*results))
clean_df['polarity_score'] = results[0]
clean_df['polarity'] = results[1]
print(clean_df['polarity'].value_counts())

# 5. Visualisasi Pie Chart proporsi sentimen
import matplotlib.pyplot as plt

fig, ax = plt.ubplots(figsize=(5,5))
counts =clean_df['polarity'].value_counts()
counts.plot.pie(autopct='%1.1f%%', ax=ax, startangle=0)
ax.set_title('Proporsi Sentimen Review Application Sea Bank')
ax.axis('equal')
plt.show()

# Cek dataset review setelah diberi label dengan Lexicon
clean_df.head()

"""# **Feature Extraction**"""

# Import kebutuhan random search dan beberapa algoritma klasifikasi
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC # Import SVC

# Pisahkan fitur dan label
X = clean_df['text_akhir']
y = clean_df['polarity']

# Tambahkan param distribusi untuk TF-IDF
tfidf_params = {
    'tfidf__max_features': [500, 1000, 2000],
    'tfidf__ngram_range': [(1,1), (1,2)],
    'tfidf__max_df': [0.7, 0.75, 1.0],
    'tfidf__min_df': [1, 2, 3]
}

# Tambahkan param distribusi untuk SVC
svc_params = {
    'clf__C': [0.1, 1, 10],
    'clf__kernel': ['linear', 'rbf']
}

# Gabungkan parameter TF-IDF dan SVC
param_distributions = {**tfidf_params, **svc_params}

# Buat pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', SVC()) # Ganti ke SVC
])

# Simpan parameter TF-IDF terbaik
best_estimators = {}

# RandomizedSearchCV
random_search = RandomizedSearchCV(
        pipeline,
        param_distributions=param_distributions, # Gunakan param_distributions gabungan
        n_iter=10,
        cv=5,
        scoring='f1_macro',
        random_state=42,
        n_jobs=-1
)

# Fit ke data teks
print("Random Search for TF-IDF and SVC...")
random_search.fit(X, y)
print(f"Best parameters: {random_search.best_params_}")
print(f"Best score: {random_search.best_score_:.4f}")
print("-" * 40)

# Gunakan parameter optimal untuk TF-IDF
tfidf = TfidfVectorizer(max_features=2000, ngram_range=(1,2), max_df=1.0, min_df=1)
X_tfidf = tfidf.fit_transform(X)

# Cek bentuk data setelah TF-IDF
X_tfidf.shape

"""# **Modeling**

## **Data Split**
"""

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)

"""## **Train Model**"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""### **Latih Model Machine Learning Classifier dengan TF-IDF**"""

# Train model Support Vector Classifier
model_svc = SVC(kernel='rbf', C=1.0, gamma='scale')
model_svc.fit(X_train.toarray(), y_train)

# Prediksi model dengan data latih dan uji
y_pred_train_svc = model_svc.predict(X_train.toarray())
y_pred_test_svc = model_svc.predict(X_test.toarray())

# Evaluasi model Support Vector Classifier
accuracy_train_svc = accuracy_score(y_pred_train_svc, y_train)
accuracy_test_svc = accuracy_score(y_pred_test_svc, y_test)
print(f"Support Vector Classifier Accuracy (Train): {accuracy_train_svc:.4f}")
print(f"Support Vector Classifier Accuracy (Test): {accuracy_test_svc:.4f}")

# Dictionary untuk menyimpan prediksi dari setiap model
# Only keeping Support Vector Classifier as requested
model_predictions = {
    'Support Vector Classifier': y_pred_test_svc
}

# Melakukan loop untuk menampilkan confusion matrix dan classification report
for model_name, predictions in model_predictions.items():
    print(f"--- {model_name} ---")
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, predictions))
    print("\nClassification Report:")
    print(classification_report(y_test, predictions))
    print("-" * 40)

# Buat dataframe untuk menyimpan hasil akurasi train dan test
accuracy_df = pd.DataFrame({
    'Model': ['Support Vector Classifier'],
    'Train Accuracy': [accuracy_train_svc],
    'Test Accuracy': [accuracy_test_svc]
})

# Tampilkan dataframe secara sort
accuracy_df = accuracy_df.sort_values(by='Test Accuracy', ascending=False)
print(accuracy_df.to_string(index=False))

# Bar chart untuk menampilkan nilai akurasi train dan test dari model
# Set posisi bar untuk masing-masing nilai (Train dan Test)
bar_width = 0.35
index = np.arange(len(accuracy_df['Model']))

fig, ax = plt.subplots(figsize=(12, 7))

# Buat bar chart untuk train accuracy
bars1 = ax.bar(index, accuracy_df['Train Accuracy'], bar_width, label='Train Accuracy', color='skyblue')

# Buat bar chart untuk test accuracy
bars2 = ax.bar(index + bar_width, accuracy_df['Test Accuracy'], bar_width, label='Test Accuracy', color='lightcoral')

ax.set_xlabel('Model')
ax.set_ylabel('Accuracy')
ax.set_title('Perbandingan Akurasi Train dan Test Model Classifier')
ax.set_xticks(index + bar_width / 2)
ax.set_xticklabels(accuracy_df['Model'], rotation=45, ha='right') # Rotasi label agar tidak bertumpuk
ax.legend()

# Set y-axis limit for accuracy (0 to 1)
ax.set_ylim(0, 1)

# Tambahkan nilai di atas bar
def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2., height,
                f'{height:.4f}',
                ha='center', va='bottom')

autolabel(bars1)
autolabel(bars2)

# Tata letak agar tidak terpotong
plt.tight_layout()

# Tampilkan plot
plt.show()

"""### **Simpan Hasil Model SVM dengan Joblib**"""

import joblib

# Simpan model Support Vector Classifier
joblib.dump(model_svc, 'svc_model.joblib')

# Simpan TfidfVectorizer
joblib.dump(tfidf, 'tfidf_vectorizer.joblib')

print("Model SVC dan TF-IDF Vectorizer telah berhasil disimpan.")